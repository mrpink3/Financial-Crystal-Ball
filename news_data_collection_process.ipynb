{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT Front Page News Collection for Financial Analysis\n",
    "\n",
    "This notebook collects New York Times front page news headlines and content from 2004-2025 for comprehensive financial market analysis.\n",
    "\n",
    "## Features\n",
    "- **Complete Coverage**: Pages 1-10 for full front page story coverage\n",
    "- **Rich Content**: Headlines, lead paragraphs, snippets, and metadata\n",
    "- **Sectioned Organization**: News organized by themes (Politics, Economics, Health, etc.)\n",
    "- **JSON Output**: Organized data files ready for analysis\n",
    "- **Historical Range**: 2004-2025 (22 years of front page news)\n",
    "\n",
    "## Quick Start\n",
    "1. Set your API key in the configuration cell\n",
    "2. Run the comprehensive collection function\n",
    "3. Data will be saved as organized JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.7.9)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS AND LIBRARIES\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "Output directory: historical_front_page_news\n",
      "API Key: 4BoKNTP1zE...ZV63Z\n",
      "Max pages: 10\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for NYT front page news collection.\"\"\"\n",
    "    api_key: str = \"4BoKNTP1zEPMKk9BG8j4eqLn94pZV63Z\"\n",
    "    output_dir: str = \"historical_front_page_news\"\n",
    "    max_pages: int = 10\n",
    "    rate_limit_delay: float = 1.0\n",
    "    base_url: str = \"https://api.nytimes.com/svc/archive/v1\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"API Key: {config.api_key[:10]}...{config.api_key[-5:]}\")\n",
    "print(f\"Max pages: {config.max_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE UTILITY FUNCTIONS\n",
    "def ensure_directory_exists(directory_path: str) -> None:\n",
    "    Path(directory_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def is_front_page_article(article: Dict, max_page: int = 1) -> bool:\n",
    "    \"\"\"Modified to focus on true front page (page 1) by default\"\"\"\n",
    "    print_page = article.get(\"print_page\")\n",
    "    print_section = article.get(\"print_section\", \"\")\n",
    "    \n",
    "    try:\n",
    "        page_number = int(print_page) if print_page else 0\n",
    "        \n",
    "        # True front page: page 1 in section A (or no section specified)\n",
    "        if page_number == 1 and (print_section == \"A\" or print_section == \"\"):\n",
    "            return True\n",
    "            \n",
    "        # Allow flexibility with max_page if needed\n",
    "        return 1 <= page_number <= max_page\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_publication_date(date_string: str) -> Optional[datetime]:\n",
    "    try:\n",
    "        return parser.parse(date_string) if date_string else None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ArticleSummary:\n",
    "    \"\"\"Structured representation of a news article.\"\"\"\n",
    "    headline: str\n",
    "    snippet: str\n",
    "    abstract: str\n",
    "    lead_paragraph: str\n",
    "    publication_date: str\n",
    "    section_name: str\n",
    "    print_page: str\n",
    "    page_section: str\n",
    "    web_url: str\n",
    "    byline: str\n",
    "\n",
    "def extract_article_data(raw_article: Dict) -> ArticleSummary:\n",
    "    \"\"\"Extract and structure article data from NYT API response.\"\"\"\n",
    "    return ArticleSummary(\n",
    "        headline=raw_article.get(\"headline\", {}).get(\"main\", \"\"),\n",
    "        snippet=raw_article.get(\"snippet\", \"\"),\n",
    "        abstract=raw_article.get(\"abstract\", \"\"),\n",
    "        lead_paragraph=raw_article.get(\"lead_paragraph\", \"\"),\n",
    "        publication_date=raw_article.get(\"pub_date\", \"\"),\n",
    "        section_name=raw_article.get(\"section_name\", \"\"),\n",
    "        print_page=raw_article.get(\"print_page\", \"\"),\n",
    "        page_section = raw_article.get(\"print_section\", \"\"),\n",
    "        web_url=raw_article.get(\"web_url\", \"\"),\n",
    "        byline=raw_article.get(\"byline\", {}).get(\"original\", \"\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def process_articles_batch(raw_articles: List[Dict]) -> List[Dict]:\n",
    "    processed_articles = []\n",
    "    for raw_article in raw_articles:\n",
    "        try:\n",
    "            article_summary = extract_article_data(raw_article)\n",
    "            # Convert dataclass to dictionary for JSON serialization\n",
    "            processed_articles.append(article_summary.__dict__)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(directory_path: str):\n",
    "    Path(directory_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED FILE OPERATIONS WITH SECTION ORGANIZATION\n",
    "def save_articles_to_json(articles: List[Dict], target_date: str, output_directory: str) -> bool:\n",
    "    \"\"\"\n",
    "    Save articles to JSON file with only the standard (flat) structure.\n",
    "    Output: { \"metadata\": { ... }, \"articles\": [ ... ] }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ensure_directory_exists(output_directory)\n",
    "        data = {\n",
    "            \"metadata\": {\n",
    "                \"date\": target_date,\n",
    "                \"total_articles\": len(articles)\n",
    "            },\n",
    "            \"articles\": articles\n",
    "        }\n",
    "        filename = f\"{target_date}.json\"\n",
    "        filepath = Path(output_directory) / filename\n",
    "        with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved standard file: {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to JSON: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###SECTION FILE STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef organize_articles_by_section(articles: List[Dict]) -> Dict[str, List[Dict]]:\\n    sections = defaultdict(list)\\n    for article in articles:\\n        section = article.get(\"section_name\", \"Unknown\") or \"Unknown\"\\n        sections[section].append(article)\\n    return dict(sections)\\n\\ndef save_articles_by_sections(articles: List[Dict], target_date: str, output_directory: str) -> bool:\\n    try:\\n        ensure_directory_exists(output_directory)\\n        organized_articles = organize_articles_by_section(articles)\\n\\n        structured_data = {\\n            \"metadata\": {\\n                \"date\": target_date,\\n                \"total_articles\": len(articles)\\n            },\\n            \"sections\": organized_articles\\n        }\\n\\n        filename = f\"news_by_sections_{target_date}.json\"\\n        filepath = Path(output_directory) / filename\\n\\n        with open(filepath, \\'w\\', encoding=\\'utf-8\\') as file:\\n            json.dump(structured_data, file, ensure_ascii=False, indent=2)\\n\\n        print(f\"Saved {len(articles)} articles grouped by section to {filepath}\")\\n        return True\\n\\n    except Exception as e:\\n        print(f\"Error saving sectioned JSON: {e}\")\\n        return False\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def organize_articles_by_section(articles: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "    sections = defaultdict(list)\n",
    "    for article in articles:\n",
    "        section = article.get(\"section_name\", \"Unknown\") or \"Unknown\"\n",
    "        sections[section].append(article)\n",
    "    return dict(sections)\n",
    "\n",
    "def save_articles_by_sections(articles: List[Dict], target_date: str, output_directory: str) -> bool:\n",
    "    try:\n",
    "        ensure_directory_exists(output_directory)\n",
    "        organized_articles = organize_articles_by_section(articles)\n",
    "\n",
    "        structured_data = {\n",
    "            \"metadata\": {\n",
    "                \"date\": target_date,\n",
    "                \"total_articles\": len(articles)\n",
    "            },\n",
    "            \"sections\": organized_articles\n",
    "        }\n",
    "\n",
    "        filename = f\"news_by_sections_{target_date}.json\"\n",
    "        filepath = Path(output_directory) / filename\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            json.dump(structured_data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Saved {len(articles)} articles grouped by section to {filepath}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving sectioned JSON: {e}\")\n",
    "        return False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_complete_daily_news(articles: List[Dict], target_date: str, output_directory: str) -> bool:\n",
    "    print(f\"SAVING DAILY NEWS FOR {target_date}\")\n",
    "    print(\"Saving standard format...\")\n",
    "    \"if u want save by sections, use save_articles_by_sections function\"\n",
    "    success = save_articles_to_json(articles, target_date, output_directory) \n",
    "    if success:\n",
    "        print(f\"SUCCESS! Standard file saved: {target_date}.json\")\n",
    "    else:\n",
    "        print(f\"FAILED! Could not save standard file.\")\n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_month_articles(api_key: str, year: int, month: int) -> List[Dict]:\n",
    "    \"\"\"Fetch all articles for a specific month from the NYT Archive API.\"\"\"\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "    params = {\"api-key\": api_key}\n",
    "    try:\n",
    "        resp = requests.get(url, params=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data.get(\"response\", {}).get(\"docs\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching articles for {year}-{month:02d}: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_articles_by_date(articles: List[Dict], target_date: str) -> List[Dict]:\n",
    "    \"\"\"Filter articles published on the target date (YYYY-MM-DD).\"\"\"\n",
    "    return [a for a in articles if a.get(\"pub_date\", \"\").startswith(target_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_news_for_date(target_date, api_key, output_dir=\"news_data\"):\n",
    "    date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    year, month = date_obj.year, date_obj.month\n",
    "\n",
    "    all_articles = fetch_month_articles(api_key, year, month)\n",
    "    front_page_articles = [a for a in all_articles if is_front_page_article(a)]\n",
    "    daily_articles = filter_articles_by_date(front_page_articles, target_date)\n",
    "    processed_articles = process_articles_batch(daily_articles)\n",
    "\n",
    "    if processed_articles:\n",
    "        save_articles_to_json(processed_articles, target_date, output_dir)\n",
    "\n",
    "    return processed_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_news_to_trading_day(pub_datetime_str, close_hour=18, close_minute=0):\n",
    "    dt = pd.to_datetime(pub_datetime_str)\n",
    "    # Hafta sonuysa Pazartesi'ye kaydır\n",
    "    if dt.weekday() >= 5:\n",
    "        days_to_monday = 7 - dt.weekday()\n",
    "        trading_day = dt + timedelta(days=days_to_monday)\n",
    "        return trading_day.strftime(\"%Y-%m-%d\")\n",
    "    # Hafta içi, kapanış sonrası ise ertesi gün (ve yine hafta sonu kontrolü):\n",
    "    if dt.hour > close_hour or (dt.hour == close_hour and dt.minute > close_minute):\n",
    "        trading_day = dt + timedelta(days=1)\n",
    "        while trading_day.weekday() >= 5:\n",
    "            trading_day += timedelta(days=1)\n",
    "        return trading_day.strftime(\"%Y-%m-%d\")\n",
    "    # Diğer durumlarda aynı gün:\n",
    "    return dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trading_date(pub_datetime_str):\n",
    "    dt = pd.to_datetime(pub_datetime_str)\n",
    "    # Eğer hafta sonuysa, bir sonraki Pazartesi'ye kaydır\n",
    "    if dt.weekday() >= 5:\n",
    "        days_to_monday = 7 - dt.weekday()\n",
    "        dt = dt + timedelta(days=days_to_monday)\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    return dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION FILTERING FOR FINANCIAL ANALYSIS\n",
    "def is_financial_relevant_section(article: Dict, include_sections=None) -> bool:\n",
    "    \"\"\"\n",
    "    Filter articles by sections relevant for financial analysis.\n",
    "    \n",
    "    Args:\n",
    "        article: Article dictionary\n",
    "        include_sections: List of sections to include. If None, uses default financial sections.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if article is in a relevant section\n",
    "    \"\"\"\n",
    "    if include_sections is None:\n",
    "        # Default financial-relevant sections\n",
    "        include_sections = [\n",
    "            \"Business Day\", \"Business\", \"Economy\", \"Economic\", \"Finance\", \"Financial\",\n",
    "            \"Markets\", \"Market\", \"Technology\", \"Tech\", \"Politics\", \"Political\",\n",
    "            \"U.S.\", \"World\", \"International\", \"Global\", \"Energy\", \"Oil\",\n",
    "            \"Federal Reserve\", \"Treasury\", \"Trade\", \"Commerce\"\n",
    "        ]\n",
    "    \n",
    "    section_name = article.get(\"section_name\", \"\").lower()\n",
    "    news_desk = article.get(\"news_desk\", \"\").lower()\n",
    "    subsection_name = article.get(\"subsection_name\", \"\").lower()\n",
    "    \n",
    "    # Check if any of the include_sections match\n",
    "    for section in include_sections:\n",
    "        section_lower = section.lower()\n",
    "        if (section_lower in section_name or \n",
    "            section_lower in news_desk or \n",
    "            section_lower in subsection_name):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Günlük Pipeline — Her haberi işlem gününe göre ilgili dosyaya kaydeder\n",
    "def collect_news_for_date(target_date, api_key, output_dir=\"news_data\"):\n",
    "    date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    year, month = date_obj.year, date_obj.month\n",
    "\n",
    "    all_articles = fetch_month_articles(api_key, year, month)\n",
    "    front_page_articles = [a for a in all_articles if is_front_page_article(a)]\n",
    "    daily_articles = filter_articles_by_date(front_page_articles, target_date)\n",
    "    financial_articles = [a for a in daily_articles if is_financial_relevant_section(a)]\n",
    "    processed_articles = process_articles_batch(financial_articles)\n",
    "\n",
    "    # --- İşlem gününe göre grupla ve kaydet ---\n",
    "    trading_day_groups = {}\n",
    "    for article in processed_articles:\n",
    "        pub_date = article.get(\"publication_date\", \"\")\n",
    "        trading_date = map_news_to_trading_day(pub_date)\n",
    "        trading_day_groups.setdefault(trading_date, []).append(article)\n",
    "    for trading_date, articles in trading_day_groups.items():\n",
    "        save_articles_to_json(articles, trading_date, output_dir)\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved standard file: historical_front_page_news\\2004-01-01.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-02.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-02.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-03.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-03.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-04.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-04.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-05.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-05.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-06.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-06.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-07.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-07.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-08.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-08.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-09.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-09.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-10.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-10.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-11.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-11.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-12.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-12.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-13.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-13.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-14.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-14.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-15.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-15.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-16.json\n",
      "Saved standard file: historical_front_page_news\\2004-01-16.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m         time.sleep(\u001b[32m12\u001b[39m)  \u001b[38;5;66;03m# API rate limit için\u001b[39;00m\n\u001b[32m     13\u001b[39m         current_date += timedelta(days=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mprocess_all_front_page_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2004\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2025\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mprocess_all_front_page_news\u001b[39m\u001b[34m(api_key, output_dir, start_year, end_year)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# API rate limit için\u001b[39;00m\n\u001b[32m     13\u001b[39m current_date += timedelta(days=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 4. Ana Toplu Çekici\n",
    "def process_all_front_page_news(api_key, output_dir=\"news_data\", start_year=2004, end_year=2025):\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        target_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        try:\n",
    "            collect_news_for_date(target_date, api_key, output_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {target_date}: {e}\")\n",
    "        time.sleep(6)  # API rate limit için\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "process_all_front_page_news(config.api_key, config.output_dir, start_year=2004, end_year=2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 HOW TO USE THE COLLECTION FUNCTION\n",
    "\n",
    "The `collect_news_for_date()` function is now ready! Here's how to use it:\n",
    "\n",
    "### Basic Usage:\n",
    "```python\n",
    "# Collect news for a specific date\n",
    "articles = collect_news_for_date('2024-12-01', config.api_key)\n",
    "```\n",
    "\n",
    "### What it does:\n",
    "1. **Fetches** all articles for the month (December 2024)\n",
    "2. **Filters** for front page articles only (pages 1-10) \n",
    "3. **Filters** for the specific date (December 1st)\n",
    "4. **Processes** articles into clean format\n",
    "5. **Saves** to JSON file: `news_data/2024-12-01.json`\n",
    "\n",
    "### Try different dates:\n",
    "```python\n",
    "# Recent dates\n",
    "articles = collect_news_for_date('2024-11-15', config.api_key)\n",
    "articles = collect_news_for_date('2024-10-31', config.api_key)\n",
    "\n",
    "# Historical dates  \n",
    "articles = collect_news_for_date('2020-03-15', config.api_key)  # COVID start\n",
    "articles = collect_news_for_date('2008-09-15', config.api_key)  # Financial crisis\n",
    "```\n",
    "\n",
    "**✅ Run the test cell below to see it in action!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
