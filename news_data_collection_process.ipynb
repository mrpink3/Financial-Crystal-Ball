{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT Front Page News Collection for Financial Analysis\n",
    "\n",
    "This notebook collects New York Times front page news headlines and content from 2004-2025 for comprehensive financial market analysis.\n",
    "\n",
    "## Features\n",
    "- **Complete Coverage**: Pages 1-10 for full front page story coverage\n",
    "- **Rich Content**: Headlines, lead paragraphs, snippets, and metadata\n",
    "- **Sectioned Organization**: News organized by themes (Politics, Economics, Health, etc.)\n",
    "- **JSON Output**: Organized data files ready for analysis\n",
    "- **Historical Range**: 2004-2025 (22 years of front page news)\n",
    "\n",
    "## Quick Start\n",
    "1. Set your API key in the configuration cell\n",
    "2. Run the comprehensive collection function\n",
    "3. Data will be saved as organized JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS AND LIBRARIES\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for NYT front page news collection.\"\"\"\n",
    "    api_key: str = \"4BoKNTP1zEPMKk9BG8j4eqLn94pZV63Z\"\n",
    "    output_dir: str = \"historical_front_page_news\"\n",
    "    max_pages: int = 10\n",
    "    rate_limit_delay: float = 1.0\n",
    "    base_url: str = \"https://api.nytimes.com/svc/archive/v1\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Output directory: {config.output_dir}\")\n",
    "print(f\"API Key: {config.api_key[:10]}...{config.api_key[-5:]}\")\n",
    "print(f\"Max pages: {config.max_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORE UTILITY FUNCTIONS\n",
    "def ensure_directory_exists(directory_path: str) -> None:\n",
    "    Path(directory_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def is_front_page_article(article: Dict, max_page: int = 1) -> bool:\n",
    "    \"\"\"Modified to focus on true front page (page 1) by default\"\"\"\n",
    "    print_page = article.get(\"print_page\")\n",
    "    print_section = article.get(\"print_section\", \"\")\n",
    "    \n",
    "    try:\n",
    "        page_number = int(print_page) if print_page else 0\n",
    "        \n",
    "        # True front page: page 1 in section A (or no section specified)\n",
    "        if page_number == 1 and (print_section == \"A\" or print_section == \"\"):\n",
    "            return True\n",
    "            \n",
    "        # Allow flexibility with max_page if needed\n",
    "        return 1 <= page_number <= max_page\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_publication_date(date_string: str) -> Optional[datetime]:\n",
    "    try:\n",
    "        return parser.parse(date_string) if date_string else None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ArticleSummary:\n",
    "    \"\"\"Structured representation of a news article.\"\"\"\n",
    "    headline: str\n",
    "    snippet: str\n",
    "    abstract: str\n",
    "    lead_paragraph: str\n",
    "    publication_date: str\n",
    "    section_name: str\n",
    "    print_page: str\n",
    "    page_section: str\n",
    "    web_url: str\n",
    "    byline: str\n",
    "\n",
    "def extract_article_data(raw_article: Dict) -> ArticleSummary:\n",
    "    \"\"\"Extract and structure article data from NYT API response.\"\"\"\n",
    "    return ArticleSummary(\n",
    "        headline=raw_article.get(\"headline\", {}).get(\"main\", \"\"),\n",
    "        snippet=raw_article.get(\"snippet\", \"\"),\n",
    "        abstract=raw_article.get(\"abstract\", \"\"),\n",
    "        lead_paragraph=raw_article.get(\"lead_paragraph\", \"\"),\n",
    "        publication_date=raw_article.get(\"pub_date\", \"\"),\n",
    "        section_name=raw_article.get(\"section_name\", \"\"),\n",
    "        print_page=raw_article.get(\"print_page\", \"\"),\n",
    "        page_section = raw_article.get(\"print_section\", \"\"),\n",
    "        web_url=raw_article.get(\"web_url\", \"\"),\n",
    "        byline=raw_article.get(\"byline\", {}).get(\"original\", \"\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def process_articles_batch(raw_articles: List[Dict]) -> List[ArticleSummary]:\n",
    "    processed_articles = []\n",
    "    for raw_article in raw_articles:\n",
    "        try:\n",
    "            article_summary = extract_article_data(raw_article)\n",
    "            processed_articles.append(article_summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "            continue\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED FILE OPERATIONS WITH SECTION ORGANIZATION\n",
    "def save_articles_to_json(articles: List[Dict], target_date: str, output_directory: str) -> bool:\n",
    "    \"\"\"\n",
    "    Save articles to JSON file with only the standard (flat) structure.\n",
    "    Output: { \"metadata\": { ... }, \"articles\": [ ... ] }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ensure_directory_exists(output_directory)\n",
    "        data = {\n",
    "            \"metadata\": {\n",
    "                \"date\": target_date,\n",
    "                \"total_articles\": len(articles)\n",
    "            },\n",
    "            \"articles\": articles\n",
    "        }\n",
    "        filename = f\"{target_date}.json\"\n",
    "        filepath = Path(output_directory) / filename\n",
    "        with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved standard file: {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to JSON: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION FILE STRUCTURE(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def organize_articles_by_section(articles: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "    sections = defaultdict(list)\n",
    "    for article in articles:\n",
    "        section = article.get(\"section_name\", \"Unknown\") or \"Unknown\"\n",
    "        sections[section].append(article)\n",
    "    return dict(sections)\n",
    "\n",
    "def save_articles_by_sections(articles: List[Dict], target_date: str, output_directory: str) -> bool:\n",
    "    try:\n",
    "        ensure_directory_exists(output_directory)\n",
    "        organized_articles = organize_articles_by_section(articles)\n",
    "\n",
    "        structured_data = {\n",
    "            \"metadata\": {\n",
    "                \"date\": target_date,\n",
    "                \"total_articles\": len(articles)\n",
    "            },\n",
    "            \"sections\": organized_articles\n",
    "        }\n",
    "\n",
    "        filename = f\"news_by_sections_{target_date}.json\"\n",
    "        filepath = Path(output_directory) / filename\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            json.dump(structured_data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Saved {len(articles)} articles grouped by section to {filepath}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving sectioned JSON: {e}\")\n",
    "        return False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_complete_daily_news(articles: List[Dict], target_date: str, output_directory: str) -> bool:\n",
    "    print(f\"SAVING DAILY NEWS FOR {target_date}\")\n",
    "    print(\"Saving standard format...\")\n",
    "    \"if u want save by sections, use save_articles_by_sections function\"\n",
    "    success = save_articles_to_json(articles, target_date, output_directory) \n",
    "    if success:\n",
    "        print(f\"SUCCESS! Standard file saved: {target_date}.json\")\n",
    "    else:\n",
    "        print(f\"FAILED! Could not save standard file.\")\n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_month_articles(api_key: str, year: int, month: int) -> List[Dict]:\n",
    "    \"\"\"Fetch all articles for a specific month from the NYT Archive API.\"\"\"\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "    params = {\"api-key\": api_key}\n",
    "    try:\n",
    "        resp = requests.get(url, params=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data.get(\"response\", {}).get(\"docs\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching articles for {year}-{month:02d}: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_articles_by_date(articles: List[Dict], target_date: str) -> List[Dict]:\n",
    "    \"\"\"Filter articles published on the target date (YYYY-MM-DD).\"\"\"\n",
    "    return [a for a in articles if a.get(\"pub_date\", \"\").startswith(target_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_news_for_date(target_date, api_key, output_dir=\"news_data\"):\n",
    "    date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    year, month = date_obj.year, date_obj.month\n",
    "\n",
    "    all_articles = fetch_month_articles(api_key, year, month)\n",
    "    front_page_articles = [a for a in all_articles if is_front_page_article(a)]\n",
    "    daily_articles = filter_articles_by_date(front_page_articles, target_date)\n",
    "    processed_articles = process_articles_batch(daily_articles)\n",
    "\n",
    "    if processed_articles:\n",
    "        save_articles_to_json(processed_articles, target_date, output_dir)\n",
    "\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_news_to_trading_day(pub_datetime_str, close_hour=16, close_minute=0):\n",
    "    dt = pd.to_datetime(pub_datetime_str)\n",
    "    # Hafta sonuysa Pazartesi'ye kaydır\n",
    "    if dt.weekday() >= 5:\n",
    "        days_to_monday = 7 - dt.weekday()\n",
    "        trading_day = dt + timedelta(days=days_to_monday)\n",
    "        return trading_day.strftime(\"%Y-%m-%d\")\n",
    "    # Hafta içi, kapanış sonrası ise ertesi gün (ve yine hafta sonu kontrolü):\n",
    "    if dt.hour > close_hour or (dt.hour == close_hour and dt.minute > close_minute):\n",
    "        trading_day = dt + timedelta(days=1)\n",
    "        while trading_day.weekday() >= 5:\n",
    "            trading_day += timedelta(days=1)\n",
    "        return trading_day.strftime(\"%Y-%m-%d\")\n",
    "    # Diğer durumlarda aynı gün:\n",
    "    return dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trading_date(pub_datetime_str):\n",
    "    dt = pd.to_datetime(pub_datetime_str)\n",
    "    # Eğer hafta sonuysa, bir sonraki Pazartesi'ye kaydır\n",
    "    if dt.weekday() >= 5:\n",
    "        days_to_monday = 7 - dt.weekday()\n",
    "        dt = dt + timedelta(days=days_to_monday)\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    return dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION FILTERING FOR FINANCIAL ANALYSIS\n",
    "def is_financial_relevant_section(article: Dict, include_sections=None) -> bool:\n",
    "    \"\"\"\n",
    "    Filter articles by sections relevant for financial analysis.\n",
    "    \n",
    "    Args:\n",
    "        article: Article dictionary\n",
    "        include_sections: List of sections to include. If None, uses default financial sections.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if article is in a relevant section\n",
    "    \"\"\"\n",
    "    if include_sections is None:\n",
    "        # Default financial-relevant sections\n",
    "        include_sections = [\n",
    "            \"Business Day\", \"Business\", \"Economy\", \"Economic\", \"Finance\", \"Financial\",\n",
    "            \"Markets\", \"Market\", \"Technology\", \"Tech\", \"Politics\", \"Political\",\n",
    "            \"U.S.\", \"World\", \"International\", \"Global\", \"Energy\", \"Oil\",\n",
    "            \"Federal Reserve\", \"Treasury\", \"Trade\", \"Commerce\"\n",
    "        ]\n",
    "    \n",
    "    section_name = article.get(\"section_name\", \"\").lower()\n",
    "    news_desk = article.get(\"news_desk\", \"\").lower()\n",
    "    subsection_name = article.get(\"subsection_name\", \"\").lower()\n",
    "    \n",
    "    # Check if any of the include_sections match\n",
    "    for section in include_sections:\n",
    "        section_lower = section.lower()\n",
    "        if (section_lower in section_name or \n",
    "            section_lower in news_desk or \n",
    "            section_lower in subsection_name):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Günlük Pipeline — Her haberi işlem gününe göre ilgili dosyaya kaydeder\n",
    "def collect_news_for_date(target_date, api_key, output_dir=\"news_data\"):\n",
    "    date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    year, month = date_obj.year, date_obj.month\n",
    "\n",
    "    all_articles = fetch_month_articles(api_key, year, month)\n",
    "    front_page_articles = [a for a in all_articles if is_front_page_article(a)]\n",
    "    daily_articles = filter_articles_by_date(front_page_articles, target_date)\n",
    "    financial_articles = [a for a in daily_articles if is_financial_relevant_section(a)]\n",
    "    processed_articles = process_articles_batch(financial_articles)\n",
    "\n",
    "    # --- İşlem gününe göre grupla ve kaydet ---\n",
    "    trading_day_groups = {}\n",
    "    for article in processed_articles:\n",
    "        pub_date = article.get(\"publication_date\", \"\")\n",
    "        trading_date = map_news_to_trading_day(pub_date)\n",
    "        trading_day_groups.setdefault(trading_date, []).append(article)\n",
    "    for trading_date, articles in trading_day_groups.items():\n",
    "        save_articles_to_json(articles, trading_date, output_dir)\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def process_all_front_page_news(api_key, output_dir=\"news_data\", start_year=2004, end_year=2025):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            print(f\"Fetching {year}-{month:02d}...\")\n",
    "            all_articles = fetch_month_articles(api_key, year, month)\n",
    "            if not all_articles:\n",
    "                continue\n",
    "            # Önce front page ve finansal filtre\n",
    "            filtered_articles = [\n",
    "                a for a in all_articles\n",
    "                if is_front_page_article(a) and is_financial_relevant_section(a)\n",
    "            ]\n",
    "            # Trading day'e göre grupla\n",
    "            trading_day_groups = defaultdict(list)\n",
    "            for a in filtered_articles:\n",
    "                pub_date = a.get(\"pub_date\", \"\")\n",
    "                trading_date = map_news_to_trading_day(pub_date)\n",
    "                article_clean = extract_article_data(a)\n",
    "                trading_day_groups[trading_date].append(asdict(article_clean))\n",
    "            # Kaydet\n",
    "            for trading_date, articles in trading_day_groups.items():\n",
    "                save_articles_to_json(articles, trading_date, output_dir)\n",
    "            time.sleep(12)  # Gerekirse azalt, rate limit'e dikkat et!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_front_page_news(config.api_key, config.output_dir, start_year=2004, end_year=2025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
